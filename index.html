<!DOCTYPE HTML>
<html>

<head>
    <title>Houwen Peng</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=1000">
    <link rel="stylesheet" href="https://use.typekit.net/quv7bsd.css"> <!-- fonts -->
    <link rel="stylesheet" href="style.css" />

    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date(); a = s.createElement(o),
                m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');
        ga('create', 'UA-89797207-1', 'auto');
        ga('send', 'pageview');

        // Toggle news function
        function toggleNews() {
            var moreNewsBtn = document.getElementById('moreNewsBtn');
            var lessNewsBtn = document.getElementById('lessNewsBtn');
            var newsList = document.querySelectorAll('.news ul li');
            console.log('newsList length:', newsList.length)

            var isExpanded = (moreNewsBtn.style.display === 'none');
            console.log('isExpanded:', isExpanded)

            // Show/hide all items after the first 5
            for (var i = 5; i < newsList.length; i++) {
                if (isExpanded) {
                    newsList[i].style.display = 'none';
                } else {
                    newsList[i].style.display = '';
                }
            }

            // Toggle buttons
            if (isExpanded) {
                moreNewsBtn.style.display = 'block';
                lessNewsBtn.style.display = 'none';
            } else {
                moreNewsBtn.style.display = 'none';
                lessNewsBtn.style.display = 'block';
            }
        }

        // Initialize news section - hide items after the first 5
        window.addEventListener('load', function () {
            var newsList = document.querySelectorAll('.news ul li');
            console.log('newsList length:', newsList.length)
            for (var i = 5; i < newsList.length; i++) {
                newsList[i].style.display = 'none';
            }

            // Initialize paper filtering system
            initPaperFiltering();
        });

        // Paper filtering functionality
        function initPaperFiltering() {
            // Create filter controls
            var researchSection = document.querySelector('.research');
            if (!researchSection) return;

            // Get all papers
            var papers = document.querySelectorAll('.research-proj');
            if (papers.length === 0) return;

            // Create filter container
            var filterContainer = document.createElement('div');
            filterContainer.className = 'paper-filter';
            filterContainer.style.margin = '20px 0 20px 30px';
            filterContainer.style.padding = '15px';
            filterContainer.style.backgroundColor = '#f5f5f5';
            filterContainer.style.borderRadius = '5px';
            filterContainer.style.display = 'flex';
            filterContainer.style.flexDirection = 'column';
            filterContainer.style.gap = '10px';

            // // Add title
            // var filterTitle = document.createElement('h3');
            // filterTitle.textContent = 'è®ºæ–‡ç­›é€‰:';
            // filterTitle.style.margin = '0';
            // filterContainer.appendChild(filterTitle);

            // Get all unique years and topics
            var years = new Set();
            var topics = new Set();
            papers.forEach(paper => {
                var year = paper.getAttribute('data-year');
                if (year) years.add(year);

                var paperTopics = paper.getAttribute('data-topics');
                if (paperTopics) {
                    paperTopics.split(',').forEach(topic => {
                        topics.add(topic.trim());
                    });
                }
            });

            // Create selected filter
            var selectedFilter = document.createElement('div');
            selectedFilter.className = 'filter-group';
            selectedFilter.innerHTML = '<strong>Selected Papers:</strong> ';

            var selectedCheckbox = document.createElement('input');
            selectedCheckbox.type = 'checkbox';
            selectedCheckbox.id = 'selected-filter';
            selectedCheckbox.style.marginLeft = '10px';

            var selectedLabel = document.createElement('label');
            selectedLabel.htmlFor = 'selected-filter';
            selectedLabel.textContent = 'Show Selected Papers';
            selectedLabel.style.marginLeft = '5px';

            selectedFilter.appendChild(selectedCheckbox);
            selectedFilter.appendChild(selectedLabel);
            filterContainer.appendChild(selectedFilter);

            // Create year filter
            var yearFilter = document.createElement('div');
            yearFilter.className = 'filter-group';
            yearFilter.innerHTML = '<strong>Filter by Year:</strong> ';

            var yearSelect = document.createElement('select');
            yearSelect.id = 'year-filter';
            yearSelect.style.marginLeft = '10px';
            yearSelect.style.padding = '5px';

            // Add default option
            var defaultOption = document.createElement('option');
            defaultOption.value = 'all';
            defaultOption.textContent = 'All Years';
            yearSelect.appendChild(defaultOption);

            // Add year options, sorted in descending order
            Array.from(years).sort((a, b) => b - a).forEach(year => {
                var option = document.createElement('option');
                option.value = year;
                option.textContent = year;
                yearSelect.appendChild(option);
            });

            yearFilter.appendChild(yearSelect);
            filterContainer.appendChild(yearFilter);

            // Create topic filter
            var topicFilter = document.createElement('div');
            topicFilter.className = 'filter-group';
            topicFilter.innerHTML = '<strong>Filter by Topic:</strong> ';

            var topicSelect = document.createElement('select');
            topicSelect.id = 'topic-filter';
            topicSelect.style.marginLeft = '10px';
            topicSelect.style.padding = '5px';

            // Add default option
            var topicDefaultOption = document.createElement('option');
            topicDefaultOption.value = 'all';
            topicDefaultOption.textContent = 'All Topics';
            topicSelect.appendChild(topicDefaultOption);

            // Add topic options, sorted alphabetically
            Array.from(topics).sort().forEach(topic => {
                var option = document.createElement('option');
                option.value = topic;
                option.textContent = topic;
                topicSelect.appendChild(option);
            });

            topicFilter.appendChild(topicSelect);
            filterContainer.appendChild(topicFilter);

            // Add filter container before the first paper
            if (papers.length > 0) {
                researchSection.insertBefore(filterContainer, papers[0]);
            }

            // Add filter event listeners
            document.getElementById('year-filter').addEventListener('change', filterPapers);
            document.getElementById('topic-filter').addEventListener('change', filterPapers);
            document.getElementById('selected-filter').addEventListener('change', filterPapers);
        }

        // Filter papers based on selected criteria
        function filterPapers() {
            var yearFilter = document.getElementById('year-filter').value;
            var topicFilter = document.getElementById('topic-filter').value;
            var selectedFilter = document.getElementById('selected-filter').checked;

            var papers = Array.from(document.querySelectorAll('.research-proj'));
            var researchSection = document.getElementById('paper');

            // Sort papers based on filter criteria
            if (yearFilter === 'all' && topicFilter === 'all') {
                // Default: sort by year descending
                papers.sort((a, b) => b.getAttribute('data-year') - a.getAttribute('data-year'));
            } else if (yearFilter !== 'all') {
                // When filtering by specific year, sort by year descending
                papers.sort((a, b) => b.getAttribute('data-year') - a.getAttribute('data-year'));
            } else if (topicFilter !== 'all') {
                // When filtering by topic, sort by topic then year
                papers.sort((a, b) => {
                    // First by topic relevance (exact match comes first)
                    const aTopics = a.getAttribute('data-topics');
                    const bTopics = b.getAttribute('data-topics');
                    const aHasTopic = aTopics && aTopics.includes(topicFilter);
                    const bHasTopic = bTopics && bTopics.includes(topicFilter);

                    if (aHasTopic && !bHasTopic) return -1;
                    if (!aHasTopic && bHasTopic) return 1;

                    // Then by year descending
                    return b.getAttribute('data-year') - a.getAttribute('data-year');
                });
            }

            // Apply filter and reorder papers
            papers.forEach(paper => {
                var paperYear = paper.getAttribute('data-year');
                var paperTopics = paper.getAttribute('data-topics');
                var isSelected = paper.getAttribute('data-select');

                var yearMatch = (yearFilter === 'all' || paperYear === yearFilter);
                var topicMatch = (topicFilter === 'all' ||
                    (paperTopics && paperTopics.includes(topicFilter)));
                var selectedMatch = (!selectedFilter || isSelected === 'True');

                if (yearMatch && topicMatch && selectedMatch) {
                    paper.style.display = '';
                    // Move to correct position
                    researchSection.appendChild(paper);
                } else {
                    paper.style.display = 'none';
                }
            });
        }
    </script>
</head>

<body id="body">
    <div id="main">
        <header id="header">
            <div class="header-container">
                <div class="header-left">
                    <span class="header-name">Houwen Peng</span>
                </div>
                <div class="header-right">
                    <a href="#home">Home</a>&nbsp;&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;
                    <a href="#news">News</a>&nbsp;&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;
                    <a href="#paper">Paper</a>&nbsp;&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;
                    <a href="#group">Group</a>&nbsp;&nbsp;&nbsp;&nbsp;/&nbsp;&nbsp;&nbsp;
                    <a href="https://shaozhenliu.github.io/test_blog.github.io/">Blog</a>
                </div>
            </div>
        </header>
        <div id="home">
            <div id="profile">
                <div id="profile-pic">
                    <img src="images/people/houwen-home1.jpg">
                    <p>
                        <a href="mailto:penghouwen@icloud.com">
                            Email
                        </a>
                        &nbsp;&nbsp;/&nbsp;&nbsp;
                        <a
                            href="https://scholar.google.com/citations?hl=zh-CN&user=UYlhQS8AAAAJ&view_op=list_works&sortby=pubdate">
                            Scholar
                        </a>
                        &nbsp;&nbsp;/&nbsp;&nbsp;
                        <a href="https://github.com/henryacm"> <!-- TODO è®°å¾—ä¿®æ”¹ä¸ºè‡ªå·±çš„githubè´¦å· -->
                            Github
                        </a>
                        &nbsp;&nbsp;/&nbsp;&nbsp;
                        <a href="https://X.com/henrypeng"> <!-- TODO è®°å¾—ä¿®æ”¹ä¸ºè‡ªå·±çš„Xè´¦å· -->
                            X
                        </a>
                    </p>
                </div>
                <div id="profile-intro">
                    <div id="profile-name">Houwen Peng <font face="STKaiti" size="5"> <span lang="zh-cn">(å½­åŽšæ–‡)</span>
                        </font>
                    </div>
                    <p>
                        Houwen is a researcher at Microsoft Research as of 2018. Before that he was a senior engineer at
                        Qualcomm AI Research. He received Ph.D. from NLPR, Instituation of Automation, Chinese Academy
                        of Sciences in 2016. From 2015 to 2016, he worked as a visiting research scholar at Temple
                        University. His main research interest is AI foundation and efficient multi-modality
                        intelligence. His research work has been shipped as core technologies to various Microsoft
                        products, including Azure, Office, Bing, Visual Studio, etc.
                    </p>
                    <p>
                        Email: penghouwen [at] icloud [dot] com <br>
                        Address: 13423 Building 2, Microsoft, No. 5 Danling St., Haidian, Beijing 100080
                    </p>
                </div>
                <div style="clear: both;"></div>
            </div>
        </div>

        <div id="news" class="section news">
            <h1>News</h1>
            <ul>
                <li>
                    We're hiring, send a free <a href="mailto:penghouwen@icloud.com">email</a>
                </li>
                <li>
                    09/2025 - <a href="https://huggingface.co/YannQi/R-4B">R-4B</a> released, a leading
                    small-scale VLM model. <a href="https://huggingface.co/YannQi/R-4B">HF</a>
                </li>


                <li>
                    05/2025 - <a href="https://evalmodels.github.io/rbenchv/">R-BenchV</a> released. <a
                        href="https://evalmodels.github.io/rbenchv/#leaderboard">Leaderboard</a>
                </li>

                <li>
                    05/2025 - <a href="https://openreview.net/pdf?id=BeoXADC9PW">R-Bench</a> accepted by ICML'2025. <a
                        href="https://evalmodels.github.io/rbench/#leaderboard">Leaderboard</a>
                </li>

                <li>
                    05/2025 - <a href="https://arxiv.org/pdf/2503.13360">Take-along VoT</a> accepted by ACL'2025. <a
                        href="https://huggingface.co/datasets/Allen8/TVC-Data">Data</a>
                </li>

                <li>
                    09/2024 - <a href="https://arxiv.org/abs/2408.08310">ScalingFilter</a> accepted by EMNLP'2024
                    main
                    conference.
                </li>

                <li>
                    05/2024 - <a href="https://arxiv.org/abs/2405.20335">Xwin-LM</a> released. <a
                        href="https://github.com/Xwin-LM/Xwin-LM">Code</a>
                </li>

                <li>
                    04/2024 - Appointed as AE for <a
                        href="https://www.sciencedirect.com/journal/pattern-recognition">Pattern Recognition</a>.
                </li>

                <li>
                    03/2024 - <a href="https://arxiv.org/abs/2403.04706">Xwin-Math</a> released, 7B small LMs are
                    approaching GPT-4 perf on math capabilities.
                </li>


                <li>
                    01/2024 - <a href="https://github.com/Xwin-LM/Xwin-LM">Xwin-LM</a> v0.3 released, ranking as the
                    top-1 open-sourced model on <a href="https://tatsu-lab.github.io/alpaca_eval/">AlpacaEval
                        benchmark</a>.
                </li>

                <li>
                    11/2023 - <a href="https://github.com/Xwin-LM/Xwin-LM">Xwin-LM</a> v0.1 ranked as the top-1
                    open-sourced model on <a href="https://tatsu-lab.github.io/alpaca_eval/">AlpacaEval
                        benchmark</a>.
                    v0.3 is coming, stay tuned.
                </li>


                <li>
                    10/2023 - <a href="https://arxiv.org/abs/2310.18313">FP8-LM</a> released.
                </li>

                <li>
                    09/2023 - <a href="https://github.com/microsoft/Cream/tree/main/TinyCLIP">TinyCLIP</a> code
                    released.
                </li>

                <li>
                    09/2023 - <a href="https://arxiv.org/abs/2308.00906">ImageBrush</a> accepted by NeurIPS'2023.
                </li>

                <li>
                    07/2023 - Three papers accepted by ICCV2023, including TinyCLIP, A-CLIP, and Efficient Tracker.
                </li>

                <li>
                    05/2023 - <a href="https://github.com/microsoft/Cream/tree/main/EfficientViT">EfficientViT</a>
                    code
                    released.
                </li>

                <li>
                    05/2023 - <a href="https://github.com/microsoft/VideoX/tree/master/SeqTrack">SeqTrack</a> code
                    released.
                </li>

                <li>
                    03/2023 - Three papers accepted by CVPR2023, including EfficientViT, SeqTrack, and iCLIP.
                </li>


                <li>
                    10/2022 - Papers with code <a href="https://paperswithcode.com/newsletter/36">Newsletter #36</a>
                    picked up <a href="https://github.com/microsoft/VideoX/tree/master/X-CLIP">X-CLIP</a>.
                </li>


                <li>
                    09/2022 - <a href="https://github.com/microsoft/Cream/tree/main/TinyViT">PointNeXt</a> accepted
                    by
                    NeurIPS'2022.
                </li>

                <li>
                    09/2022 - <a href="https://arxiv.org/pdf/2208.02816.pdf">X-CLIP</a> has been integrated into <a
                        href="https://huggingface.co/docs/transformers/model_doc/xclip">ðŸ¤— Hugging Face</a>.
                </li>

                <li>
                    09/2022 - Papers with code <a href="https://paperswithcode.com/newsletter/33">Newsletter #33</a>
                    picked up <a href="https://arxiv.org/pdf/2206.04670.pdf">PointNeXt</a>.
                </li>

                <li>
                    09/2022 - <a href="https://github.com/microsoft/Cream/tree/main/AutoFormer">AutoFormerV2</a> was
                    integrated into <a href="https://github.com/rwightman/pytorch-image-models">Timm</a>, while <a
                        href="https://github.com/researchmm/Stark"> STARK</a> was integrated into <a
                        href="https://github.com/open-mmlab/mmtracking">OpenMMTracking</a>.
                </li>

                <li>
                    08/2022 - <a href="https://github.com/microsoft/Cream/tree/main/TinyViT">TinyViT</a> is selected
                    as
                    one of the popular Github projects in July. <a
                        href="https://www.reddit.com/r/MachineLearning/comments/wi05tg/d_most_popular_ai_research_july_2022_pt_2_ranked/">Reddit</a>
                    and <a href="https://mp.weixin.qq.com/s/qGRUkt5glw2CiANCT2_3mw">XinZhiYuan</a>.
                </li>

                <li>
                    08/2022 - <a href="https://github.com/microsoft/VideoX/tree/master/X-CLIP">X-CLIP</a> released.
                </li>

                <li>
                    08/2022 - <a href="https://github.com/microsoft/Cream/tree/main/TinyViT">TinyViT</a> released.
                </li>

                <li>
                    07/2022 - A <a href="https://mp.weixin.qq.com/s/reppBhb__4jeQTsPH5eg5g">post</a> on our recent
                    tiny
                    and efficient models.
                </li>


                <li>
                    07/2022 - Two papers accepted by ECCV2022, in which <a
                        href="https://arxiv.org/pdf/2208.02816.pdf">X-CLIP</a> was finally selected as an ORAL.
                </li>

                <li>
                    06/2022 - An interesting work on 3D point representation learning <a
                        href="https://arxiv.org/pdf/2206.04670.pdf">PointNeXt</a>.
                </li>

                <li>
                    04/2022 - Our work <a href="https://arxiv.org/pdf/2204.07154.pdf">MiniViT</a> was accepted to <a
                        href="https://cvpr2022.thecvf.com/">CVPR 2022</a>. <a
                        href="https://github.com/microsoft/Cream/tree/main/MiniViT">Code</a> is available.
                </li>

                <li>
                    02/2022 - Our <a href="https://arxiv.org/abs/2006.10724">CDARTS</a> is finally accepted by <a
                        href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>. CDARTS <a
                        href="https://github.com/microsoft/Cream/tree/main/CDARTS">Detection</a> and <a
                        href="https://github.com/microsoft/Cream/tree/main/CDARTS">Segmentation</a> code is
                    available
                    now.
                </li>

                <li>
                    01/2022 - Invited as an Area Chair of <a href="https://2022.acmmm.org/">ACM Multimedia 2022</a>.
                </li>

                <li>
                    12/2021 - Our visual grounding work <a
                        href="https://ieeexplore.ieee.org/abstract/document/9580623">MS-2D-TAN</a> was reported by
                    <a href="https://mp.weixin.qq.com/s/ptg-CKoCm0WMpPCpMVAGvA">1</a>, <a
                        href="https://www.sohu.com/a/512044757_129720">2</a>, <a
                        href="https://cloud.tencent.com/developer/article/1926574">3</a>, <a
                        href="https://mp.weixin.qq.com/s/v_pJHtkFFejwB7OBUMlOrA">4</a> ....
                </li>


                <li>
                    11/2021 - Our <a
                        href="https://openaccess.thecvf.com/content/ICCV2021/papers/Yan_Learning_Spatio-Temporal_Transformer_for_Visual_Tracking_ICCV_2021_paper.pdf">STARK</a>
                    tracker achieved the <a href="http://www.votchallenge.net/vot2021/program.html"> Winner</a> in
                    VOT2021 challenge. <a href="https://github.com/researchmm/Stark"> Code</a> released.
                </li>

                <li>
                    10/2021 - Our video grounding work, <a href="https://github.com/microsoft/2D-TAN">
                        MS-2D-TAN</a>,
                    accepted by TPAMI. Congrats to Songyang!
                </li>

                <li>
                    09/2021 - <a href="https://github.com/microsoft/Cream/tree/main/AutoFormer">AutoFormerV2</a>
                    accepted by NeurIPS'21. Two papers got in, congrats!
                </li>

                <li>
                    08/2021 - AutoFormer <a href="https://github.com/microsoft/Cream/tree/main/AutoFormer">code</a>
                    is
                    now released. Reported by <a
                        href="https://ai-scholar.tech/en/articles/transformer/AutoFormer">1</a>, <a
                        href="https://www.163.com/dy/article/GFR3FGAB05118HA4.html">2</a>, <a
                        href="https://www.leiphone.com/category/academic/S4mNLoLQL06YlfFC.html">3</a>, <a
                        href="https://mp.weixin.qq.com/s?src=11&timestamp=1629727482&ver=3270&signature=-tEiEiNIEkGbS4DBcOlU*mpi*2r3zA3HK2M1ymMiyhn3QOVXyW7VUDxXxa0BDXBxrmVN5uc3M449diFDZQ7u88e8EH2QVWsVGH*fewoQpK3vwM3XSNsgFumvsBxvx0RU&new=1">4</a>
                    ...
                </li>

                <li>
                    08/2021 - Invited as a senior program committee (SPC) member for <a
                        href="https://aaai.org/Conferences/AAAI-22/aaai22call">AAAI 2022</a>.
                </li>

                <li>
                    07/2021 - Four papers accepted by <a href="http://iccv2021.thecvf.com">ICCV 2021</a>.
                </li>

                <li>
                    07/2021 - Our image relative position encoding <a href="https://arxiv.org/abs/2107.00651">iRPE</a>
                    accepted by ICCV 2021, <a href="https://github.com/microsoft/AutoML/tree/main/iRPE">Code</a> and
                    <a href="https://mp.weixin.qq.com/s/TWpOOT79LLSWpq5_DB-ykQ">Blog</a>.
                </li>

                <li>
                    07/2021 - Our work on NAS vision transformer <a
                        href="https://arxiv.org/abs/2107.00651">AutoFormer</a> is available on <a
                        href="https://arxiv.org/abs/2107.00651">ArXiv</a>.
                </li>

                <li>
                    04/2021 - Appointed as Area Chair of <a href="https://2021.acmmm.org/">ACM Multimedia 2021</a>.
                </li>

                <li>
                    04/2021 - A simple spatio-temporal transformer tracker <a
                        href=" https://arxiv.org/abs/2103.17154">STARK</a> is released. <a
                        href="https://github.com/researchmm/Stark">Code</a> and <a
                        href="https://www.msra.cn/zh-cn/news/features/transformer-in-vision">Blog</a>.
                </li>

                <li>
                    03/2021 - Two works on NAS (<a href="https://arxiv.org/abs/2104.00597">EnsembleNAS</a> and <a
                        href="https://github.com/researchmm/LightTrack"> LightTrack</a>) accepted by <a
                        href="http://cvpr2021.thecvf.com/">CVPR 2021</a>.
                </li>

                <li>
                    12/2020 - <a
                        href="https://papers.nips.cc/paper/2020/file/d072677d210ac4c03ba046120f0802ec-Paper.pdf">CreamNAS</a>
                    is integrated into Microsoft <a
                        href="https://github.com/microsoft/nni/blob/master/docs/en_US/NAS/Cream.rst">NNI 2.0</a>
                    AutoML
                    platform.
                </li>

                <li>
                    11/2020 - Our <a
                        href="https://papers.nips.cc/paper/2020/file/d072677d210ac4c03ba046120f0802ec-Paper.pdf">Cream</a>
                    NAS work was accepted by NeurIPS2020. <a href="https://github.com/microsoft/Cream">Code</a> and
                    <a
                        href="https://www.msra.cn/zh-cn/news/features/neurips-2020-distilling-prioritized-paths-for-one-shot-nas">Blog</a>.
                </li>

                <li>
                    07/2020 - <a href="https://arxiv.org/abs/2006.10721">Ocean</a> is accepted by ECCV2020. <a
                        href="https://github.com/researchmm/TracKit">Code</a> is available.
                </li>


                <li>
                    06/2020 - <a href="https://arxiv.org/abs/2006.10724">CDARTS</a> has integrated into Microsoft <a
                        href="https://github.com/microsoft/nni">NNI</a> AutoML platform.
                </li>

                <li>
                    06/2020 - We released a cyclic differentiable neural architecture search algorithm, <a
                        href="https://arxiv.org/abs/2006.10724">CDARTS</a> and <a
                        href="https://github.com/researchmm/CDARTS">Code</a>.
                </li>

                <li>
                    06/2020 - We released the object-aware anchor-free tracking algorithm (Ocean), <a
                        href="https://arxiv.org/abs/2006.10721">arXiv</a> and <a
                        href="https://github.com/researchmm/TracKit">Code</a>.
                </li>


                <li>
                    04/2020 - A simple transductive video object segmentation approach, dubbed <a
                        href="https://arxiv.org/pdf/2004.07193.pdf">TVOS</a>, was accepted by CVPR2020. <a
                        href="https://github.com/microsoft/transductive-vos.pytorch">Code</a> was released!
                </li>

                <li>
                    10/2019 - A coauthor paper on video moment localization, dubbed <a
                        href="https://arxiv.org/pdf/1912.03590.pdf">2D-TAN</a>, was accepted by AAAI2020. <a
                        href="https://github.com/microsoft/2D-TAN">Code</a> and <a
                        href="https://www.msra.cn/zh-cn/news/features/aaai-2020-2d-tan">Blog </a>.
                </li>

                <li>
                    09/2019 - Our team achieved <a href="http://hacs.csail.mit.edu/challenge2019.html"> Rank #1 </a>
                    in
                    HACS Temporal Action Localization Challenge at ICCV2019 workshop.
                </li>


                <li>
                    07/2019 - Our team achieved one <a
                        href="http://www.votchallenge.net/vot2019/program.html">Winner</a> and two Runner-ups in
                    VOT2019
                    challenges. <a href="https://github.com/researchmm/SiamDW"> Code</a> was released.
                </li>

                <li>
                    03/2019 - Our first tracking paper, dubbed <a
                        href="http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Deeper_and_Wider_Siamese_Networks_for_Real-Time_Visual_Tracking_CVPR_2019_paper.html">SiamDW</a>,
                    was accepted by CVPR2019 as an Oral. <a href="https://github.com/researchmm/SiamDW">Code</a> is
                    available.
                </li>

                <div onclick="toggleNews()" id="moreNewsBtn" class="showBtn"><a>show more</a></div>
                <div onclick="toggleNews()" id="lessNewsBtn" class="showBtn"><a>show less</a></div>
            </ul>
            <div style="clear: both;"></div>
        </div>

        <div class="divider"></div>

        <div id="paper" class="section research">
            <h1>Publications</h1>

            <div class="research-proj" data-year="2023" data-topics="Computer Vision, Foundation Models"
                data-select="True">
                <a href="https://github.com/yannqi/R-4B" class="research-thumb">
                    <img src="./images/projects/R-4B.png" alt="" />
                </a>
                <a href="https://github.com/yannqi/R-4B" class="research-proj-title">R-4B: Incentivizing General-Purpose
                    Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning</a>
                <div class="paper-meta">
                    <span class="paper-year">2023</span>
                    <span class="paper-topics">Computer Vision, Foundation Models</span>
                    <span class="selected-badge">Selected</span>
                </div>
                <p> Qi Yang, Bolin Ni, Shiming Xiang, Han Hu, Houwen Peng, Jie Jiang <br>
                    Tech Report <br>
                    <a href="https://arxiv.org/pdf/2508.21113">Tech Report</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/yannqi/R-4B">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://huggingface.co/YannQi/R-4B">HF</a>
                </p>
            </div>


            <div class="research-proj" data-year="2015"
                data-topics="Multimodal Learning, Neural Architecture Search, Transformer Architecture"
                data-select="False">
                <a href="https://evalmodels.github.io/rbenchv/" class="research-thumb">
                    <img src="./images/projects/R-BenchV.png" alt="" />
                </a>
                <a href="https://evalmodels.github.io/rbenchv/" class="research-proj-title">R-BenchV: A Primary
                    Assessment for Visual Reasoning Models with Multi-modal Outputs</a>
                <div class="paper-meta">
                    <span class="paper-year">2015</span>
                    <span class="paper-topics">Multimodal Learning, Neural Architecture Search, Transformer
                        Architecture</span>

                </div>
                <p> Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-lin Li, Xinjie Lin, Jinnian
                    Zhang, Xin-Sheng Chen, Yi Zhang, Kiyohiro Nakayama, Zhengyang Geng, Houwen Peng, Han Hu, Shi-min Hu
                    <br>
                    Tech Report <br>
                    <a href="https://arxiv.org/pdf/2505.16770">Tech Report</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://evalmodels.github.io/rbenchv/">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://evalmodels.github.io/rbenchv/#leaderboard">Leaderboard</a>
                </p>
            </div>


            <div class="research-proj" data-year="2020"
                data-topics="Transformer Architecture, Natural Language Processing, Computer Vision, Large Language Models"
                data-select="True">
                <a href="https://openreview.net/pdf?id=BeoXADC9PW" class="research-thumb">
                    <img src="./images/projects/R-Bench.png" alt="" />
                </a>
                <a href="https://openreview.net/pdf?id=BeoXADC9PW" class="research-proj-title">R-Bench: Graduate-level
                    Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation</a>
                <div class="paper-meta">
                    <span class="paper-year">2020</span>
                    <span class="paper-topics">Transformer Architecture, Natural Language Processing, Computer Vision,
                        Large Language Models</span>
                    <span class="selected-badge">Selected</span>
                </div>
                <p> Meng-Hao Guo, Jiajun Xu, Yi Zhang, Jiaxi Song, Haoyang Peng, Yi-Xuan Deng, Xinzhi Dong, Kiyohiro
                    Nakayama, Zhengyang Geng, Chen Wang, Bolin Ni, Yongming Rao, Houwen Peng, Han Hu, Gordon Wetzstein,
                    Shi-min Hu <br>
                    ICML 2025 <br>
                    <a href="https://openreview.net/pdf?id=BeoXADC9PW">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://evalmodels.github.io/rbench">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://evalmodels.github.io/rbenchv/#leaderboard">Leaderboard</a>
                </p>
            </div>


            <div class="research-proj" data-year="2023"
                data-topics="Transformer Architecture, Computer Vision, Foundation Models" data-select="True">
                <a href="https://arxiv.org/pdf/2503.13360" class="research-thumb">
                    <img src="./images/projects/ACL25.png" alt="" />
                </a>
                <a href="https://arxiv.org/pdf/2503.13360" class="research-proj-title">Mitigating Visual Forgetting via
                    Take-along Visual Conditioning for Multi-modal Long CoT Reasoning</a>
                <div class="paper-meta">
                    <span class="paper-year">2023</span>
                    <span class="paper-topics">Transformer Architecture, Computer Vision, Foundation Models</span>
                    <span class="selected-badge">Selected</span>
                </div>
                <p> Hai-Long Sun, Zhun Sun, Houwen Peng, Han-Jia Ye <br>
                    ACL 2025 <br>
                    <a href="https://arxiv.org/pdf/2503.13360">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://sun-hailong.github.io/projects/TVC/">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2025" data-topics="Neural Architecture Search, Multimodal Learning"
                data-select="False">
                <a href="https://arxiv.org/pdf/2408.08310" class="research-thumb">
                    <img src="./images/projects/ScalingFilter.png" alt="" />
                </a>
                <a href="https://arxiv.org/pdf/2408.08310" class="research-proj-title">ScalingFilter: Assessing Data
                    Quality through Inverse Utilization of Scaling Laws</a>
                <div class="paper-meta">
                    <span class="paper-year">2025</span>
                    <span class="paper-topics">Neural Architecture Search, Multimodal Learning</span>

                </div>
                <p> Ruihang Li, <a
                        href="https://scholar.google.com/citations?user=xwudKb4AAAAJ&amp;hl=en&amp;oi=ao">Yixuan
                        Wei</a>, Miaosen Zhang, Nenghai Yu, <a href="https://ancientmooner.github.io/">Han Hu</a>,
                    Houwen Peng <br>
                    EMNLP 2024 <br>
                    <a href="https://arxiv.org/pdf/2408.08310">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/scalingfilter">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2015" data-topics="Model Compression, Visual Tracking"
                data-select="False">
                <a href="https://github.com/Xwin-LM/Xwin-LM" class="research-thumb">
                    <img src="./images/projects/Xwin-LM.png" alt="" />
                </a>
                <a href="https://github.com/Xwin-LM/Xwin-LM" class="research-proj-title">Xwin-LM: Strong and Scalable
                    Alignment Practice for LLMs</a>
                <div class="paper-meta">
                    <span class="paper-year">2015</span>
                    <span class="paper-topics">Model Compression, Visual Tracking</span>

                </div>
                <p> Bolin Ni, Jingcheng Hu, <a
                        href="https://scholar.google.com/citations?user=xwudKb4AAAAJ&amp;hl=en&amp;oi=ao">Yixuan
                        Wei</a>, Houwen Peng, <a href="https://stupidzz.github.io/">Zheng Zhang</a>, <a
                        href="https://people.ucas.edu.cn/~gfmeng?language=en">Gaofeng Meng</a>, <a
                        href="https://ancientmooner.github.io/">Han Hu</a> <br>
                    Tech Report <br>
                    <a href="https://arxiv.org/pdf/2405.20335">Tech Report</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/Xwin-LM/Xwin-LM">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2019"
                data-topics="Multimodal Learning, Transformer Architecture, Natural Language Processing"
                data-select="False">
                <a href="https://github.com/Xwin-LM/Xwin-LM" class="research-thumb">
                    <img src="./images/projects/Xwin-Math.png" alt="" />
                </a>
                <a href="https://github.com/Xwin-LM/Xwin-LM" class="research-proj-title">Common 7B Language Models
                    Already Possess Strong Math Capabilities</a>
                <div class="paper-meta">
                    <span class="paper-year">2019</span>
                    <span class="paper-topics">Multimodal Learning, Transformer Architecture, Natural Language
                        Processing</span>

                </div>
                <p> Chen Li, Weiqi Wang, Jingcheng Hu, <a
                        href="https://scholar.google.com/citations?user=xwudKb4AAAAJ&amp;hl=en&amp;oi=ao">Yixuan
                        Wei</a>, Nanning Zheng, <a href="https://ancientmooner.github.io/">Han Hu</a>, <a
                        href="https://stupidzz.github.io/">Zheng Zhang</a>, Houwen Peng <br>
                    Tech Report <br>
                    <a href="https://arxiv.org/abs/2403.04706">Tech Report</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/Xwin-LM/Xwin-LM">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2022" data-topics="Computer Vision, Natural Language Processing"
                data-select="False">
                <a href="https://github.com/Azure/MS-AMP" class="research-thumb">
                    <img src="./images/projects/FP8-LM.png" alt="" />
                </a>
                <a href="https://github.com/Azure/MS-AMP" class="research-proj-title">FP8-LM: Training FP8 Large
                    Language Models</a>
                <div class="paper-meta">
                    <span class="paper-year">2022</span>
                    <span class="paper-topics">Computer Vision, Natural Language Processing</span>

                </div>
                <p> Houwen Peng, <a
                        href="https://scholar.google.com/citations?user=sK4JUL4AAAAJ&amp;hl=zh-CN&amp;oi=sra">Kan
                        Wu</a>, <a
                        href="https://scholar.google.com/citations?user=xwudKb4AAAAJ&amp;hl=en&amp;oi=ao">Yixuan
                        Wei</a>, Guoshuai Zhao, Yuxiang Yang, <a href="https://zeliu98.github.io/">Ze Liu</a>, Yifan
                    Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, Ruihang Li, Miaosen Zhang, Chen Li, Jia Ning, Ruizhe
                    Wang, <a href="https://stupidzz.github.io/">Zheng Zhang</a>, Shuguang Liu, <a
                        href="https://ancientmooner.github.io/">Han Hu</a>, <a
                        href="https://www.microsoft.com/en-us/research/people/pengc/">Peng Cheng</a> <br>
                    Tech Report <br>
                    <a href="https://arxiv.org/abs/2310.18313">Tech Report</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/Azure/MS-AMP">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2018"
                data-topics="Model Compression, Computer Vision, Natural Language Processing, Neural Architecture Search"
                data-select="False">
                <a href="https://arxiv.org/abs/2308.00906" class="research-thumb">
                    <img src="./images/projects/ImageBrush.png" alt="" />
                </a>
                <a href="https://arxiv.org/abs/2308.00906" class="research-proj-title">ImageBrush: Learning Visual
                    In-Context Instructions for Exemplar-Based Image Manipulation</a>
                <div class="paper-meta">
                    <span class="paper-year">2018</span>
                    <span class="paper-topics">Model Compression, Computer Vision, Natural Language Processing, Neural
                        Architecture Search</span>

                </div>
                <p> <a href="">Yasheng Sun</a>, <a href="">Yifan Yang</a>, Houwen Peng*, <a href="">Yifei Shen</a>, <a
                        href="">Yuqing Yang</a>, <a href="https://ancientmooner.github.io/">Han Hu</a>, <a
                        href="https://scholar.google.com/citations?user=16posrQAAAAJ&amp;hl=en&amp;oi=ao/">Lili Qiu</a>,
                    <a href="">Hideki Koike</a> <br>
                    NeurIPS 2023 <br>
                    <a href="https://arxiv.org/abs/2308.00906">Paper</a>
                </p>
            </div>


            <div class="research-proj" data-year="2025"
                data-topics="Foundation Models, Computer Vision, Multimodal Learning, Visual Tracking"
                data-select="False">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_TinyCLIP_CLIP_Distillation_via_Affinity_Mimicking_and_Weight_Inheritance_ICCV_2023_paper.pdf"
                    class="research-thumb">
                    <img src="./images/projects/TinyCLIP.png" alt="" />
                </a>
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_TinyCLIP_CLIP_Distillation_via_Affinity_Mimicking_and_Weight_Inheritance_ICCV_2023_paper.pdf"
                    class="research-proj-title">TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight
                    Inheritance</a>
                <div class="paper-meta">
                    <span class="paper-year">2025</span>
                    <span class="paper-topics">Foundation Models, Computer Vision, Multimodal Learning, Visual
                        Tracking</span>

                </div>
                <p> <a href="https://scholar.google.com/citations?user=sK4JUL4AAAAJ&amp;hl=zh-CN&amp;oi=sra">Kan Wu</a>,
                    Houwen Peng*, <a href="">Zhenghong Zhou</a>, <a
                        href="https://www.microsoft.com/en-us/research/people/bixi/">Bin Xiao</a>, <a
                        href="https://scholar.google.com/citations?user=cOPQtYgAAAAJ&amp;hl=zh-CN&amp;oi=ao">Mengchen
                        Liu</a>, <a href="https://www.microsoft.com/en-us/research/people/luyuan/">Lu Yuan</a>, <a
                        href="">Hong Xuan</a>, <a href="">Zhenghong Zhou</a>, <a href="">Xi Chen</a>, <a
                        href="https://xwcv.github.io/">Xinggang Wang</a>, Hongyang Chao, <a
                        href="https://ancientmooner.github.io/">Han Hu</a> <br>
                    ICCV 2023 <br>
                    <a
                        href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_TinyCLIP_CLIP_Distillation_via_Affinity_Mimicking_and_Weight_Inheritance_ICCV_2023_paper.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/microsoft/Cream">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2025"
                data-topics="Foundation Models, Natural Language Processing, Efficient AI" data-select="False">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Attentive_Mask_CLIP_ICCV_2023_paper.pdf"
                    class="research-thumb">
                    <img src="./images/projects/A-CLIP.png" alt="" />
                </a>
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Attentive_Mask_CLIP_ICCV_2023_paper.pdf"
                    class="research-proj-title">Attentive Mask CLIP</a>
                <div class="paper-meta">
                    <span class="paper-year">2025</span>
                    <span class="paper-topics">Foundation Models, Natural Language Processing, Efficient AI</span>

                </div>
                <p> <a href="">Yifan Yang</a>, <a href="">Weiquan Huang</a>, <a
                        href="https://scholar.google.com/citations?user=xwudKb4AAAAJ&amp;hl=en&amp;oi=ao">Yixuan
                        Wei</a>, Houwen Peng*, <a href="">Xinyang Jiang</a>, <a href="">Huiqiang Jiang</a>, <a
                        href="">Fangyun Wei</a>, <a href="">Yin Wang</a>, <a href="https://ancientmooner.github.io/">Han
                        Hu</a>, <a
                        href="https://scholar.google.com/citations?user=16posrQAAAAJ&amp;hl=en&amp;oi=ao/">Lili Qiu</a>,
                    <a href="">Yuqing Yang</a> <br>
                    ICCV 2023 <br>
                    <a
                        href="https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Attentive_Mask_CLIP_ICCV_2023_paper.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/microsoft/A-CLIP">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2025"
                data-topics="Model Compression, Neural Architecture Search, Transformer Architecture"
                data-select="True">
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kang_Exploring_Lightweight_Hierarchical_Vision_Transformers_for_Efficient_Visual_Tracking_ICCV_2023_paper.pdf"
                    class="research-thumb">
                    <img src="./images/projects/HIT.png" alt="" />
                </a>
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kang_Exploring_Lightweight_Hierarchical_Vision_Transformers_for_Efficient_Visual_Tracking_ICCV_2023_paper.pdf"
                    class="research-proj-title">Exploring Lightweight Hierarchical Vision Transformers for Efficient
                    Visual Tracking</a>
                <div class="paper-meta">
                    <span class="paper-year">2025</span>
                    <span class="paper-topics">Model Compression, Neural Architecture Search, Transformer
                        Architecture</span>
                    <span class="selected-badge">Selected</span>
                </div>
                <p> <a href="">Ben Kang</a>, <a href="">Xin Chen</a>, <a
                        href="https://scholar.google.com/citations?user=nVgPQpoAAAAJ&amp;hl=ja&amp;oi=ao">Dong Wang</a>,
                    Houwen Peng*, <a href="http://ice.dlut.edu.cn/lu/index.html">Huchuan Lu</a> <br>
                    ICCV 2023 <br>
                    <a
                        href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kang_Exploring_Lightweight_Hierarchical_Vision_Transformers_for_Efficient_Visual_Tracking_ICCV_2023_paper.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/kangben258/HiT">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2017"
                data-topics="Computer Vision, Large Language Models, Visual Tracking, Model Compression"
                data-select="True">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_EfficientViT_Memory_Efficient_Vision_Transformer_With_Cascaded_Group_Attention_CVPR_2023_paper.pdf"
                    class="research-thumb">
                    <img src="./images/projects/EfficientVit.gif" alt="" />
                </a>
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_EfficientViT_Memory_Efficient_Vision_Transformer_With_Cascaded_Group_Attention_CVPR_2023_paper.pdf"
                    class="research-proj-title">EfficientViT: Memory Efficient Vision Transformer with Cascaded Group
                    Attention</a>
                <div class="paper-meta">
                    <span class="paper-year">2017</span>
                    <span class="paper-topics">Computer Vision, Large Language Models, Visual Tracking, Model
                        Compression</span>
                    <span class="selected-badge">Selected</span>
                </div>
                <p> <a href="https://xinyuliu-jeffrey.github.io/">Xinyu Liu</a>, Houwen Peng*, <a href="">Ningxin
                        Zheng</a>, <a href="">Yuqing Yang</a>, <a href="https://ancientmooner.github.io/">Han Hu</a>, <a
                        href="http://www.ee.cuhk.edu.hk/~yxyuan/">Yixuan Yuan</a> <br>
                    CVPR 2023 <br>
                    <a
                        href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_EfficientViT_Memory_Efficient_Vision_Transformer_With_Cascaded_Group_Attention_CVPR_2023_paper.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/microsoft/Cream/tree/main/EfficientViT">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2017"
                data-topics="Model Compression, Natural Language Processing, Multimodal Learning" data-select="True">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_SeqTrack_Sequence_to_Sequence_Learning_for_Visual_Object_Tracking_CVPR_2023_paper.pdf"
                    class="research-thumb">
                    <img src="./images/projects/SeqTrack.png" alt="" />
                </a>
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_SeqTrack_Sequence_to_Sequence_Learning_for_Visual_Object_Tracking_CVPR_2023_paper.pdf"
                    class="research-proj-title">SeqTrack: Sequence to Sequence Learning for Visual Object Tracking</a>
                <div class="paper-meta">
                    <span class="paper-year">2017</span>
                    <span class="paper-topics">Model Compression, Natural Language Processing, Multimodal
                        Learning</span>
                    <span class="selected-badge">Selected</span>
                </div>
                <p> <a href="">Xin Chen</a>, Houwen Peng*, <a
                        href="https://scholar.google.com/citations?user=nVgPQpoAAAAJ&amp;hl=ja&amp;oi=ao">Dong Wang</a>,
                    <a href="http://ice.dlut.edu.cn/lu/index.html">Huchuan Lu</a>, <a
                        href="https://ancientmooner.github.io/">Han Hu</a> <br>
                    CVPR 2023 <br>
                    <a
                        href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_SeqTrack_Sequence_to_Sequence_Learning_for_Visual_Object_Tracking_CVPR_2023_paper.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/microsoft/VideoX">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2018" data-topics="Foundation Models, Efficient AI"
                data-select="False">
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_iCLIP_Bridging_Image_Classification_and_Contrastive_Language-Image_Pre-Training_for_Visual_CVPR_2023_paper.pdf"
                    class="research-thumb">
                    <img src="./images/projects/iCLIP.png" alt="" />
                </a>
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_iCLIP_Bridging_Image_Classification_and_Contrastive_Language-Image_Pre-Training_for_Visual_CVPR_2023_paper.pdf"
                    class="research-proj-title">iCLIP: Bridging Image Classification and Contrastive Language-Image
                    Pre-Training for Visual Recognition</a>
                <div class="paper-meta">
                    <span class="paper-year">2018</span>
                    <span class="paper-topics">Foundation Models, Efficient AI</span>

                </div>
                <p> <a href="https://scholar.google.com/citations?user=xwudKb4AAAAJ&amp;hl=en&amp;oi=ao">Yixuan Wei</a>,
                    <a href="http://yue-cao.me/"></a>Yue Cao, <a href="https://stupidzz.github.io/">Zheng Zhang</a>,
                    Houwen Peng, <a href="">Zhuliang Yao</a>, <a href="https://zdaxie.github.io/">Zhenda Xie</a>, <a
                        href="https://ancientmooner.github.io/">Han Hu</a>, <a
                        href="https://www.microsoft.com/en-us/research/people/bainguo/">Baining Guo</a> <br>
                    CVPR 2023 <br>
                    <a
                        href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wei_iCLIP_Bridging_Image_Classification_and_Contrastive_Language-Image_Pre-Training_for_Visual_CVPR_2023_paper.pdf">Paper</a>
                </p>
            </div>


            <div class="research-proj" data-year="2016"
                data-topics="Multimodal Learning, Efficient AI, Model Compression" data-select="False">
                <a href="https://arxiv.org/pdf/2206.04670.pdf" class="research-thumb">
                    <img src="./images/projects/PointNeXt.png" alt="" />
                </a>
                <a href="https://arxiv.org/pdf/2206.04670.pdf" class="research-proj-title">PointNeXt: Revisiting
                    PointNet++ with Improved Training and Scaling Strategies</a>
                <div class="paper-meta">
                    <span class="paper-year">2016</span>
                    <span class="paper-topics">Multimodal Learning, Efficient AI, Model Compression</span>

                </div>
                <p> <a href="">Guocheng Qian</a>, <a href="">Yuchen Li</a>, Houwen Peng*, <a href="">Jinjie Mai</a>, <a
                        href="./"> Hasan Abed Al Kader Hammoud</a>, <a
                        href="https://people.ucas.edu.cn/~gfmeng?language=en"> Mohamed Elhoseiny</a>, <a
                        href="https://jianlong-fu.github.io/">Mohamed Elhoseiny</a>, <a
                        href="https://people.ucas.ac.cn/~xiangshiming">Bernard Ghanem*</a> <br>
                    NeurIPS 2022 <br>
                    <a href="https://arxiv.org/pdf/2206.04670.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/guochengqian/pointnext">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2020"
                data-topics="Large Language Models, Computer Vision, Visual Tracking" data-select="False">
                <a href="https://arxiv.org/pdf/2208.02816.pdf" class="research-thumb">
                    <img src="./images/projects/X-CLIP.PNG" alt="" />
                </a>
                <a href="https://arxiv.org/pdf/2208.02816.pdf" class="research-proj-title">Expanding Language-Image
                    Pretrained Models for General Video Recognition</a>
                <div class="paper-meta">
                    <span class="paper-year">2020</span>
                    <span class="paper-topics">Large Language Models, Computer Vision, Visual Tracking</span>

                </div>
                <p> <a href="">Bolin Ni</a>, Houwen Peng*, <a href="">Minghao Chen</a>, <a
                        href="https://sy-zhang.github.io/">Songyang Zhang</a>, <a
                        href="https://people.ucas.edu.cn/~gfmeng?language=en">Gaofeng Meng</a>, <a
                        href="https://jianlong-fu.github.io/">Jianlong Fu</a>, <a
                        href="https://people.ucas.ac.cn/~xiangshiming">Shiming Xiang</a>, <a
                        href="https://www3.cs.stonybrook.edu/~hling/">Haibin Ling</a> <br>
                    ECCV 2022 <br>
                    <strong> Oral Presentation </strong>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://arxiv.org/pdf/2208.02816.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://aka.ms/X-CLIP">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://huggingface.co/docs/transformers/model_doc/xclip">ðŸ¤— Hugging Face</a>
                </p>
            </div>


            <div class="research-proj" data-year="2023"
                data-topics="Natural Language Processing, Efficient AI, Foundation Models, Large Language Models"
                data-select="False">
                <a href="https://arxiv.org/pdf/2207.10666.pdf" class="research-thumb">
                    <img src="./images/projects/TinyViT2.png" alt="" />
                </a>
                <a href="https://arxiv.org/pdf/2207.10666.pdf" class="research-proj-title">TinyViT: Fast Pretraining
                    Distillation for Small Vision Transformers</a>
                <div class="paper-meta">
                    <span class="paper-year">2023</span>
                    <span class="paper-topics">Natural Language Processing, Efficient AI, Foundation Models, Large
                        Language Models</span>

                </div>
                <p> <a href="https://scholar.google.com/citations?user=sK4JUL4AAAAJ&amp;hl=zh-CN&amp;oi=sra">Kan Wu</a>,
                    <a href="https://scholar.google.com/citations?user=tbd4Tf4AAAAJ&amp;hl=zh-CN&amp;oi=ao">Jinnian
                        Zhang</a>, Houwen Peng*, <a
                        href="https://scholar.google.com/citations?user=cOPQtYgAAAAJ&amp;hl=zh-CN&amp;oi=ao">Mengchen
                        Liu</a>, <a href="https://www.microsoft.com/en-us/research/people/bixi/">Bin Xiao</a>, <a
                        href="https://jianlong-fu.github.io/">Jianlong Fu</a>, <a
                        href="https://www.microsoft.com/en-us/research/people/luyuan/">Lu Yuan</a> <br>
                    ECCV 2022 <br>
                    <a href="https://arxiv.org/pdf/2207.10666.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/microsoft/Cream/tree/main/TinyViT">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2022"
                data-topics="Visual Tracking, Foundation Models, Transformer Architecture, Model Compression"
                data-select="False">
                <a href="https://arxiv.org/pdf/2204.07154.pdf" class="research-thumb">
                    <img src="./images/projects/MiniViT.png" alt="" />
                </a>
                <a href="https://arxiv.org/pdf/2204.07154.pdf" class="research-proj-title">MiniViT: Compressing Vision
                    Transformers with Weight Multiplexing</a>
                <div class="paper-meta">
                    <span class="paper-year">2022</span>
                    <span class="paper-topics">Visual Tracking, Foundation Models, Transformer Architecture, Model
                        Compression</span>

                </div>
                <p> <a href="https://scholar.google.com/citations?user=tbd4Tf4AAAAJ&amp;hl=zh-CN&amp;oi=ao">Jinnian
                        Zhang</a>, Houwen Peng*, <a
                        href="https://scholar.google.com/citations?user=sK4JUL4AAAAJ&amp;hl=zh-CN&amp;oi=sra">Kan
                        Wu</a>, <a
                        href="https://scholar.google.com/citations?user=cOPQtYgAAAAJ&amp;hl=zh-CN&amp;oi=ao">Mengchen
                        Liu</a>, <a href="https://www.microsoft.com/en-us/research/people/bixi/">Bin Xiao</a>, <a
                        href="https://jianlong-fu.github.io/">Jianlong Fu</a>, <a
                        href="https://www.microsoft.com/en-us/research/people/luyuan/">Lu Yuan</a> <br>
                    CVPR 2022 <br>
                    <a href="https://arxiv.org/pdf/2204.07154.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/microsoft/Cream/tree/main/MiniViT">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2016"
                data-topics="Multimodal Learning, Visual Tracking, Computer Vision" data-select="False">
                <a href="https://arxiv.org/abs/2006.10724" class="research-thumb">
                    <img src="./images/projects/arXiv20-CDARTS.PNG" alt="" />
                </a>
                <a href="https://arxiv.org/abs/2006.10724" class="research-proj-title">Cyclic Differentiable
                    Architecture Search</a>
                <div class="paper-meta">
                    <span class="paper-year">2016</span>
                    <span class="paper-topics">Multimodal Learning, Visual Tracking, Computer Vision</span>

                </div>
                <p> <a href="">Hongyuan Yu</a>, Houwen Peng*, <a href="">Yan Huang</a>, <a href="">Hao Du</a>, <a
                        href="https://jianlong-fu.github.io/">Jianlong Fu</a>, <a
                        href="http://cripac.ia.ac.cn/people/lwang/M-MCG/people.html">Liang Wang</a>, <a
                        href="https://www3.cs.stonybrook.edu/~hling/">Haibin Ling</a> <br>
                    TPAMI 2022 <br>
                    <a href="https://arxiv.org/abs/2006.10724">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/microsoft/Cream">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2016" data-topics="Visual Tracking, Natural Language Processing"
                data-select="True">
                <a href="https://arxiv.org/pdf/2012.02646.pdf" class="research-thumb">
                    <img src="./images/projects/MS-2DTAN.PNG" alt="" />
                </a>
                <a href="https://arxiv.org/pdf/2012.02646.pdf" class="research-proj-title">Multi-Scale 2D Temporal
                    Adjacent Networks for Moment Localization with Natural Language</a>
                <div class="paper-meta">
                    <span class="paper-year">2016</span>
                    <span class="paper-topics">Visual Tracking, Natural Language Processing</span>
                    <span class="selected-badge">Selected</span>
                </div>
                <p> <a href="https://sy-zhang.github.io/">Songyang Zhang</a>, Houwen Peng*, <a
                        href="https://jianlong-fu.github.io/">Jianlong Fu</a>, <a
                        href="https://scholar.google.com/citations?user=cpkrT44AAAAJ&amp;hl=en&amp;oi=ao">Yijuan Lu</a>,
                    <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a> <br>
                    TPAMI 2021 <br>
                    <a href="https://arxiv.org/pdf/2012.02646.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/microsoft/2D-TAN">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2022" data-topics="Neural Architecture Search, Computer Vision"
                data-select="False">
                <a href="https://papers.nips.cc/paper/2021/hash/48e95c45c8217961bf6cd7696d80d238-Abstract.html"
                    class="research-thumb">
                    <img src="./images/projects/AutoFormerV2.PNG" alt="" />
                </a>
                <a href="https://papers.nips.cc/paper/2021/hash/48e95c45c8217961bf6cd7696d80d238-Abstract.html"
                    class="research-proj-title">AutoFormerV2: Searching the Search Space of Vision Transformer</a>
                <div class="paper-meta">
                    <span class="paper-year">2022</span>
                    <span class="paper-topics">Neural Architecture Search, Computer Vision</span>

                </div>
                <p> Minghao Chen, Kan Wu, Bolin Ni, Houwen Peng*, Bei Liu, <a
                        href="https://jianlong-fu.github.io/">Jianlong Fu</a>, Hongyang Chao, <a
                        href="https://www3.cs.stonybrook.edu/~hling/">Haibin Ling</a> <br>
                    NeurIPS 2021 <br>
                    <a
                        href="https://papers.nips.cc/paper/2021/hash/48e95c45c8217961bf6cd7696d80d238-Abstract.html">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/microsoft/AutoML">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2023" data-topics="Neural Architecture Search, Large Language Models"
                data-select="True">
                <a href="https://houwenpeng.com" class="research-thumb">
                    <img src="./images/projects/VLP.PNG" alt="" />
                </a>
                <a href="https://houwenpeng.com" class="research-proj-title">Probing Inter-modality: Visual Parsing with
                    Self-Attention for Vision-and-Language Pre-training</a>
                <div class="paper-meta">
                    <span class="paper-year">2023</span>
                    <span class="paper-topics">Neural Architecture Search, Large Language Models</span>
                    <span class="selected-badge">Selected</span>
                </div>
                <p> Hongwei Xue, Yupan Huang, Bei Liu, Houwen Peng, <a href="https://jianlong-fu.github.io/">Jianlong
                        Fu</a>, Houqiang Li, <a href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a> <br>
                    NeurIPS 2021 <br>
                    <a href="https://houwenpeng.com">Paper</a>
                </p>
            </div>


            <div class="research-proj" data-year="2022" data-topics="Large Language Models, Model Compression"
                data-select="True">
                <a href="https://arxiv.org/abs/2108.12711" class="research-thumb">
                    <img src="./images/projects/USOT.PNG" alt="" />
                </a>
                <a href="https://arxiv.org/abs/2108.12711" class="research-proj-title">Learning to Track Objects from
                    Unlabled Videos</a>
                <div class="paper-meta">
                    <span class="paper-year">2022</span>
                    <span class="paper-topics">Large Language Models, Model Compression</span>
                    <span class="selected-badge">Selected</span>
                </div>
                <p> Jilai Zheng, <a href="https://vision.sjtu.edu.cn/index.html">Chao Ma</a>, Houwen Peng, <a
                        href="https://scholar.google.com/citations?user=nVgPQpoAAAAJ&amp;hl=ja&amp;oi=ao">Xiaokang
                        Yang</a> <br>
                    ICCV 2021 <br>
                    <a href="https://arxiv.org/abs/2108.12711">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/VISION-SJTU/USOT">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2020"
                data-topics="Large Language Models, Model Compression, Transformer Architecture" data-select="False">
                <a href="./publications/iRPE.pdf" class="research-thumb">
                    <img src="./images/projects/iRPE.png" alt="" />
                </a>
                <a href="./publications/iRPE.pdf" class="research-proj-title">Rethinking and Improving Relative Position
                    Encoding for Vision Transformer</a>
                <div class="paper-meta">
                    <span class="paper-year">2020</span>
                    <span class="paper-topics">Large Language Models, Model Compression, Transformer Architecture</span>

                </div>
                <p> Kan Wu, Houwen Peng*, Minghao Chen, <a href="https://jianlong-fu.github.io/">Jianlong Fu</a>, <a
                        href="https://www3.cs.stonybrook.edu/~hling/">Hongyang Chao</a> <br>
                    ICCV 2021 <br>
                    <a href="./publications/iRPE.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/microsoft/AutoML">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2023"
                data-topics="Neural Architecture Search, Computer Vision, Multimodal Learning, Foundation Models"
                data-select="False">
                <a href="https://arxiv.org/pdf/2107.00651.pdf" class="research-thumb">
                    <img src="./images/projects/AutoFormerV2.gif" alt="" />
                </a>
                <a href="https://arxiv.org/pdf/2107.00651.pdf" class="research-proj-title">AutoFormer: Searching
                    Transformers for Visual Recognition</a>
                <div class="paper-meta">
                    <span class="paper-year">2023</span>
                    <span class="paper-topics">Neural Architecture Search, Computer Vision, Multimodal Learning,
                        Foundation Models</span>

                </div>
                <p> Minghao Chen, Houwen Peng*, <a href="https://jianlong-fu.github.io/">Jianlong Fu</a>, <a
                        href="https://www3.cs.stonybrook.edu/~hling/">Haibin Ling</a> <br>
                    ICCV 2021 <br>
                    <a href="https://arxiv.org/pdf/2107.00651.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/microsoft/AutoML">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2015"
                data-topics="Multimodal Learning, Visual Tracking, Natural Language Processing, Large Language Models"
                data-select="False">
                <a href="https://arxiv.org/abs/2103.17154" class="research-thumb">
                    <img src="./images/projects/STARK.png" alt="" />
                </a>
                <a href="https://arxiv.org/abs/2103.17154" class="research-proj-title">Learning Spatio-Temporal
                    Transformer for Visual Tracking</a>
                <div class="paper-meta">
                    <span class="paper-year">2015</span>
                    <span class="paper-topics">Multimodal Learning, Visual Tracking, Natural Language Processing, Large
                        Language Models</span>

                </div>
                <p> <a href="https://scholar.google.com/citations?user=3f8qn4cAAAAJ&amp;hl=ja&amp;oi=ao">Bin Yan</a>,
                    Houwen Peng*, Jianlong Fu, <a
                        href="https://scholar.google.com/citations?user=nVgPQpoAAAAJ&amp;hl=ja&amp;oi=ao">Dong Wang</a>,
                    <a href="http://ice.dlut.edu.cn/lu/index.html">Huchuan Lu</a> <br>
                    ICCV 2021 <br>
                    <a href="https://arxiv.org/abs/2103.17154">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/researchmm/Stark">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2025"
                data-topics="Transformer Architecture, Multimodal Learning, Foundation Models, Natural Language Processing"
                data-select="True">
                <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yan_LightTrack_Finding_Lightweight_Neural_Networks_for_Object_Tracking_via_One-Shot_CVPR_2021_paper.pdf"
                    class="research-thumb">
                    <img src="./images/projects/CVPR21-LightTrack.PNG" alt="" />
                </a>
                <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yan_LightTrack_Finding_Lightweight_Neural_Networks_for_Object_Tracking_via_One-Shot_CVPR_2021_paper.pdf"
                    class="research-proj-title">LightTrack: Finding Lightweight Neural Networks for Object Tracking via
                    One-Shot Architecture Search</a>
                <div class="paper-meta">
                    <span class="paper-year">2025</span>
                    <span class="paper-topics">Transformer Architecture, Multimodal Learning, Foundation Models, Natural
                        Language Processing</span>
                    <span class="selected-badge">Selected</span>
                </div>
                <p> <a href="https://scholar.google.com/citations?user=3f8qn4cAAAAJ&amp;hl=ja&amp;oi=ao">Bin
                        Yan<sup>â€ </sup></a>, Houwen Peng<sup>â€ </sup>, Kan Wu<sup>â€ </sup>, <a
                        href="https://scholar.google.com/citations?user=nVgPQpoAAAAJ&amp;hl=ja&amp;oi=ao">Dong Wang</a>,
                    <a href="https://jianlong-fu.github.io/">Jianlong Fu</a>, <a
                        href="http://ice.dlut.edu.cn/lu/index.html">Huchuan Lu</a> <br>
                    CVPR 2021 <br>
                    <a
                        href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yan_LightTrack_Finding_Lightweight_Neural_Networks_for_Object_Tracking_via_One-Shot_CVPR_2021_paper.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/researchmm/LightTrack">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2023" data-topics="Efficient AI, Computer Vision" data-select="True">
                <a href="https://arxiv.org/abs/2104.00597" class="research-thumb">
                    <img src="./images/projects/CVPR21-NEAS.PNG" alt="" />
                </a>
                <a href="https://arxiv.org/abs/2104.00597" class="research-proj-title">One-Shot Neural Ensemble
                    Architecture Search by Diversity-Guided Search Space Shrinking</a>
                <div class="paper-meta">
                    <span class="paper-year">2023</span>
                    <span class="paper-topics">Efficient AI, Computer Vision</span>
                    <span class="selected-badge">Selected</span>
                </div>
                <p> Minghao Chen, <a href="https://jianlong-fu.github.io/">Jianlong Fu</a>, <a
                        href="https://www3.cs.stonybrook.edu/~hling/">Haibin Ling</a> <br>
                    CVPR 2021 <br>
                    <a href="https://arxiv.org/abs/2104.00597">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/researchmm/NEAS">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2020" data-topics="Multimodal Learning, Foundation Models"
                data-select="False">
                <a href="https://papers.nips.cc/paper/2020/file/d072677d210ac4c03ba046120f0802ec-Paper.pdf"
                    class="research-thumb">
                    <img src="./images/projects/method_.png" alt="" />
                </a>
                <a href="https://papers.nips.cc/paper/2020/file/d072677d210ac4c03ba046120f0802ec-Paper.pdf"
                    class="research-proj-title">Cream of the Crop: Distilling Prioritized Paths For One-Shot Neural
                    Architecture Search</a>
                <div class="paper-meta">
                    <span class="paper-year">2020</span>
                    <span class="paper-topics">Multimodal Learning, Foundation Models</span>

                </div>
                <p> Houwen Peng, Hao Du, <a
                        href="https://scholar.google.com/citations?user=yfnvzxYAAAAJ&amp;hl=ja&amp;oi=ao">Hongyuan
                        Yu</a>, Qi Li, <a href="https://liaojing.github.io/html/">Jing Liao</a>, <a
                        href="https://jianlong-fu.github.io/">Jianlong Fu</a> <br>
                    NeurIPS 2020 <br>
                    <a
                        href="https://papers.nips.cc/paper/2020/file/d072677d210ac4c03ba046120f0802ec-Paper.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/microsoft/Cream">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2024" data-topics="Transformer Architecture, Large Language Models"
                data-select="False">
                <a href="https://arxiv.org/pdf/2006.10721.pdf" class="research-thumb">
                    <img src="./images/projects/ECCV20-Ocean.PNG" alt="" />
                </a>
                <a href="https://arxiv.org/pdf/2006.10721.pdf" class="research-proj-title">Ocean: Object-aware
                    Anchor-free Tracking</a>
                <div class="paper-meta">
                    <span class="paper-year">2024</span>
                    <span class="paper-topics">Transformer Architecture, Large Language Models</span>

                </div>
                <p> <a href="https://scholar.google.com/citations?user=7Ws0QHYAAAAJ&amp;hl=EN">Zhipeng Zhang</a>, Houwen
                    Peng*, <a href="https://jianlong-fu.github.io/">Jianlong Fu</a>, <a
                        href="https://scholar.google.com/citations?user=tddnkNYAAAAJ&amp;hl=EN/">Bing Li</a>, <a
                        href="hhttp://people.ucas.ac.cn/~huweiming">Weiming Hu</a> <br>
                    ECCV 2020 <br>
                    <a href="https://arxiv.org/pdf/2006.10721.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/researchmm/TracKit">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2023" data-topics="Efficient AI, Foundation Models"
                data-select="False">
                <a href="https://arxiv.org/pdf/2004.07193.pdf" class="research-thumb">
                    <img src="./images/projects/CVPR20-TVOS.png" alt="" />
                </a>
                <a href="https://arxiv.org/pdf/2004.07193.pdf" class="research-proj-title">A Transductive Approach for
                    Semi-Supervised Video Object Segmentation</a>
                <div class="paper-meta">
                    <span class="paper-year">2023</span>
                    <span class="paper-topics">Efficient AI, Foundation Models</span>

                </div>
                <p> <a href="https://www.linkedin.com/in/yizhuoz">Yizhuo Zhang<sup>â€ </sup></a>, <a
                        href="https://www.microsoft.com/en-us/research/people/wuzhiron/">Zhirong Wu<sup>â€ </sup></a>,
                    Houwen Peng, <a href="https://www.microsoft.com/en-us/research/people/stevelin/">Stephen Lin</a>
                    <br>
                    CVPR 2020 <br>
                    <a href="https://arxiv.org/pdf/2004.07193.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/microsoft/transductive-vos.pytorch">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2018"
                data-topics="Computer Vision, Model Compression, Neural Architecture Search, Multimodal Learning"
                data-select="False">
                <a href="https://arxiv.org/pdf/1912.03590.pdf" class="research-thumb">
                    <img src="./images/projects/AAAI20-2D-TAN.PNG" alt="" />
                </a>
                <a href="https://arxiv.org/pdf/1912.03590.pdf" class="research-proj-title">Learning 2D Temporal
                    Localization Networks for Moment Localization with Natural Language</a>
                <div class="paper-meta">
                    <span class="paper-year">2018</span>
                    <span class="paper-topics">Computer Vision, Model Compression, Neural Architecture Search,
                        Multimodal Learning</span>

                </div>
                <p> <a href="https://sy-zhang.github.io/">Songyang Zhang</a>, Houwen Peng*, <a
                        href="https://jianlong-fu.github.io/">Jianlong Fu</a>, <a
                        href="https://www.cs.rochester.edu/u/jluo/">Jiebo Luo</a> <br>
                    AAAI 2020 <br>
                    <a href="https://arxiv.org/pdf/1912.03590.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/microsoft/2D-TAN">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2024"
                data-topics="Efficient AI, Computer Vision, Large Language Models, Multimodal Learning"
                data-select="True">
                <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Deeper_and_Wider_Siamese_Networks_for_Real-Time_Visual_Tracking_CVPR_2019_paper.html"
                    class="research-thumb">
                    <img src="./images/projects/CVPR19-SiamDW.gif" alt="" />
                </a>
                <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Deeper_and_Wider_Siamese_Networks_for_Real-Time_Visual_Tracking_CVPR_2019_paper.html"
                    class="research-proj-title">Deeper and Wider Siamese Networks for Real-time Visual Tracking</a>
                <div class="paper-meta">
                    <span class="paper-year">2024</span>
                    <span class="paper-topics">Efficient AI, Computer Vision, Large Language Models, Multimodal
                        Learning</span>
                    <span class="selected-badge">Selected</span>
                </div>
                <p> <a href="https://scholar.google.com/citations?user=7Ws0QHYAAAAJ&amp;hl=EN">Zhipeng Zhang</a>, Houwen
                    Peng* <br>
                    CVPR 2019 <br>
                    <strong> Oral Presentation </strong>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a
                        href="http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Deeper_and_Wider_Siamese_Networks_for_Real-Time_Visual_Tracking_CVPR_2019_paper.html">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://github.com/researchmm/SiamDW">Code</a>
                </p>
            </div>


            <div class="research-proj" data-year="2024"
                data-topics="Multimodal Learning, Neural Architecture Search, Computer Vision" data-select="False">
                <a href="https://dl.acm.org/doi/abs/10.1145/3343031.3350609" class="research-thumb">
                    <img src="./images/projects/MM19.PNG" alt="" />
                </a>
                <a href="https://dl.acm.org/doi/abs/10.1145/3343031.3350609" class="research-proj-title">AI Coach: Deep
                    Human Pose Estimation and Analysis for Personalized Athletic Training Assistance</a>
                <div class="paper-meta">
                    <span class="paper-year">2024</span>
                    <span class="paper-topics">Multimodal Learning, Neural Architecture Search, Computer Vision</span>

                </div>
                <p> <a href="">Jianbo Wang</a>, <a
                        href="https://scholar.google.com/citations?user=PFiDkUoAAAAJ&amp;hl=en">Kai Qiu</a>, Houwen
                    Peng, <a href="https://jianlong-fu.github.io/">Jianlong Fu</a>, <a
                        href="https://scholar.google.com/citations?user=SC-WmzwAAAAJ&amp;hl=EN">Jianke Zhu</a> <br>
                    ACM Multimedia 2019 <br>
                    <a href="https://dl.acm.org/doi/abs/10.1145/3343031.3350609">Paper</a>
                </p>
            </div>


            <div class="research-proj" data-year="2019" data-topics="Natural Language Processing, Large Language Models"
                data-select="False">
                <a href="http://www.dcs.bbk.ac.uk/~sjmaybank/MultiView.pdf" class="research-thumb">
                    <img src="./images/projects/PAMI17-LiBing.PNG" alt="" />
                </a>
                <a href="http://www.dcs.bbk.ac.uk/~sjmaybank/MultiView.pdf" class="research-proj-title">Multi-view
                    Multi-instance Learning based on Joint Sparse Representation and Multi-view Dictionary Learning</a>
                <div class="paper-meta">
                    <span class="paper-year">2019</span>
                    <span class="paper-topics">Natural Language Processing, Large Language Models</span>

                </div>
                <p> <a href="https://scholar.google.com/citations?user=tddnkNYAAAAJ&amp;hl=EN/">Bing Li</a>, <a
                        href="https://scholar.google.com/citations?user=JsxBjD0AAAAJ&amp;hl=en/">Weihua Xiong</a>,
                    Houwen Peng, <a href="hhttp://people.ucas.ac.cn/~huweiming">Weiming Hu</a>, <a
                        href="http://www.dcs.bbk.ac.uk/~sjmaybank/">Stephen J. Maybank</a> <br>
                    TPAMI 2017 <br>
                    <a href="http://www.dcs.bbk.ac.uk/~sjmaybank/MultiView.pdf">Paper</a>
                </p>
            </div>


            <div class="research-proj" data-year="2017"
                data-topics="Natural Language Processing, Neural Architecture Search" data-select="True">
                <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Illumination_Estimation_Based_2013_CVPR_paper.pdf"
                    class="research-thumb">
                    <img src="./images/projects/CVPR13.PNG" alt="" />
                </a>
                <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Illumination_Estimation_Based_2013_CVPR_paper.pdf"
                    class="research-proj-title">Illumination Estimation based on Bilayer Sparse Coding</a>
                <div class="paper-meta">
                    <span class="paper-year">2017</span>
                    <span class="paper-topics">Natural Language Processing, Neural Architecture Search</span>
                    <span class="selected-badge">Selected</span>
                </div>
                <p> <a href="https://scholar.google.com/citations?user=tddnkNYAAAAJ&amp;hl=EN/">Bing Li</a>, <a
                        href="https://scholar.google.com/citations?user=JsxBjD0AAAAJ&amp;hl=en/">Weihua Xiong</a>, <a
                        href="hhttp://people.ucas.ac.cn/~huweiming">Weiming Hu</a>, Houwen Peng <br>
                    CVPR 2013 <br>
                    <a
                        href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Li_Illumination_Estimation_Based_2013_CVPR_paper.pdf">Paper</a>
                </p>
            </div>


            <div class="research-proj" data-year="2016"
                data-topics="Foundation Models, Computer Vision, Large Language Models, Efficient AI"
                data-select="False">
                <a href="https://eprints.bbk.ac.uk/14986/1/14986.pdf" class="research-thumb">
                    <img src="./images/projects/PAMI17-SMD.jpg" alt="" />
                </a>
                <a href="https://eprints.bbk.ac.uk/14986/1/14986.pdf" class="research-proj-title">Salient Object
                    Detection via Structured Matrix Decomposition</a>
                <div class="paper-meta">
                    <span class="paper-year">2016</span>
                    <span class="paper-topics">Foundation Models, Computer Vision, Large Language Models, Efficient
                        AI</span>

                </div>
                <p> Houwen Peng, <a href="https://www3.cs.stonybrook.edu/~hling/">Haibin Ling</a>, <a
                        href="https://scholar.google.com/citations?user=tddnkNYAAAAJ&amp;hl=EN/">Bing Li</a>, <a
                        href="https://scholar.google.com/citations?user=JsxBjD0AAAAJ&amp;hl=en/">Weihua Xiong</a>, <a
                        href="hhttp://people.ucas.ac.cn/~huweiming">Weiming Hu</a>, <a
                        href="http://www.dcs.bbk.ac.uk/~sjmaybank/">Stephen J. Maybank</a> <br>
                    TPAMI 2017 <br>
                    <strong> Featured Paper </strong>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://eprints.bbk.ac.uk/14986/1/14986.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a
                        href="https://sites.google.com/site/salientobjectdetection/need-to-knows">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://sites.google.com/site/salientobjectdetection/home">Project Webpage (Updated)</a>
                </p>
            </div>


            <div class="research-proj" data-year="2021"
                data-topics="Neural Architecture Search, Large Language Models, Computer Vision, Natural Language Processing"
                data-select="True">
                <a href="./publications/MM15.pdf" class="research-thumb">
                    <img src="./images/projects/MM15.PNG" alt="" />
                </a>
                <a href="./publications/MM15.pdf" class="research-proj-title">Predicting Image Memorability by
                    Multi-view Adaptive Regression</a>
                <div class="paper-meta">
                    <span class="paper-year">2021</span>
                    <span class="paper-topics">Neural Architecture Search, Large Language Models, Computer Vision,
                        Natural Language Processing</span>
                    <span class="selected-badge">Selected</span>
                </div>
                <p> Houwen Peng, <a href="">Kai Li</a>, <a
                        href="https://scholar.google.com/citations?user=tddnkNYAAAAJ&amp;hl=EN/">Bing Li</a>, <a
                        href="https://www3.cs.stonybrook.edu/~hling/">Haibin Ling</a>, <a
                        href="https://scholar.google.com/citations?user=JsxBjD0AAAAJ&amp;hl=en/">Weihua Xiong</a>, <a
                        href="hhttp://people.ucas.ac.cn/~huweiming">Weiming Hu</a> <br>
                    ACM Multimedia 2015 <br>
                    <a href="./publications/MM15.pdf">Paper</a>
                </p>
            </div>


            <div class="research-proj" data-year="2023" data-topics="Computer Vision, Efficient AI, Multimodal Learning"
                data-select="False">
                <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-10578-9_7.pdf"
                    class="research-thumb">
                    <img src="./images/projects/ECCV14-RGBD.png" alt="" />
                </a>
                <a href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-10578-9_7.pdf"
                    class="research-proj-title">RGBD Salient Object Detection: A Benchmark and Algorithms</a>
                <div class="paper-meta">
                    <span class="paper-year">2023</span>
                    <span class="paper-topics">Computer Vision, Efficient AI, Multimodal Learning</span>

                </div>
                <p> Houwen Peng, <a href="https://scholar.google.com/citations?user=tddnkNYAAAAJ&amp;hl=EN/">Bing
                        Li</a>, <a href="https://scholar.google.com/citations?user=JsxBjD0AAAAJ&amp;hl=en/">Weihua
                        Xiong</a>, <a href="hhttp://people.ucas.ac.cn/~huweiming">Weiming Hu</a>, <a
                        href="https://people.eecs.berkeley.edu/~janner/">Rongrong Ji</a> <br>
                    ECCV 2014 <br>
                    <a
                        href="https://link.springer.com/content/pdf/10.1007%2F978-3-319-10578-9_7.pdf">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://sites.google.com/site/rgbdsaliency/code">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://sites.google.com/site/rgbdsaliency/home">Project Webpage</a>
                </p>
            </div>


            <div class="research-proj" data-year="2021" data-topics="Computer Vision, Foundation Models"
                data-select="False">
                <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI13/paper/view/6290/7270" class="research-thumb">
                    <img src="./images/projects/AAAI13-LSMD.PNG" alt="" />
                </a>
                <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI13/paper/view/6290/7270"
                    class="research-proj-title">Salient Object Detection via Low-rank and Structured Sparse Matrix
                    Decomposition</a>
                <div class="paper-meta">
                    <span class="paper-year">2021</span>
                    <span class="paper-topics">Computer Vision, Foundation Models</span>

                </div>
                <p> Houwen Peng, <a href="https://scholar.google.com/citations?user=tddnkNYAAAAJ&amp;hl=EN/">Bing
                        Li</a>, <a href="https://people.eecs.berkeley.edu/~janner/">Rongrong Ji</a>, <a
                        href="https://people.eecs.berkeley.edu/~janner/">Weiming Hu</a>, <a
                        href="https://scholar.google.com/citations?user=JsxBjD0AAAAJ&amp;hl=en/">Weihua Xiong</a> <br>
                    AAAI 2013 <br>
                    <strong> Oral Presentation </strong>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a
                        href="https://www.aaai.org/ocs/index.php/AAAI/AAAI13/paper/view/6290/7270">Paper</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a
                        href="https://sites.google.com/site/salientobjectdetection/need-to-knows">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                    <a href="https://sites.google.com/site/salientobjectdetection/home">Project Webpage (Updated)</a>
                </p>
            </div>
        </div>


        <div class="divider"></div>

        <div id="group" class="section group">
            <h1>Group</h1>

            <div class="title">Current and Former Interns/Collaborators</div>
            <ul class="two-columns">
                <li><a href="">Kan Wu</a> (PhD, Sun Yat-sen Univ.)</li>
                <li><a href="https://xinyuliu-jeffrey.github.io/">Xinyu Liu</a> (PhD, CUHK)</li>
                <li><a href="https://www.linkedin.com/in/%E8%BF%9B%E5%B9%B4-%E5%BC%A0-67141712a">Xin Chen</a> (PhD,
                    DLUT)</li>
                <li><a href="https://www.linkedin.com/in/%E8%BF%9B%E5%B9%B4-%E5%BC%A0-67141712a">Jinnian Zhang</a>
                    (PhD, UWâ€“Madison)</li>
                <li><a href="">Bolin</a> (PhD, UCAS)</li>
                <li><a href="https://houwenpeng.com/group.html">Minghao Chen</a> (PhD, Stony Brook Univ.)</li>
                <li><a href="">Bin Yan</a> (PhD, DLUT)</li>
                <li><a href="">Peiyao Wang</a> (PhD, Stony Brook Univ.)</li>
                <li><a href="">Hao Du</a> (BE, City Univ. of HK)</li>
                <li><a href="https://houwenpeng.com/group.html">Qi Li</a> (BE, Tsinghua Univ.)</li>
                <li><a href="https://sy-zhang.github.io/">Songyang Zhang</a> (PhD, Univ. of Rochester)</li>
                <li><a href="https://scholar.google.com/citations?user=ONfT7GMAAAAJ&hl=zh-CN">Le Yang</a> (PhD,
                    NWPU)</li>
                <li><a href="https://houwenpeng.com/group.html">Xingyu Ren</a> (PhD, SJTU)</li>
                <li><a href="">Hongyuan Yu</a> (PhD, CAS)</li>
                <li><a href="https://scholar.google.com/citations?user=7Ws0QHYAAAAJ&hl=EN">Zhipeng Zhang</a> (PhD,
                    CAS)</li>
            </ul>
        </div>

    </div>

</body>

</html>